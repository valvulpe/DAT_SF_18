{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 11 Introduction to Decision Trees\n",
    "\n",
    "- Dat_SF_18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we begin:\n",
    "\n",
    "\n",
    "**Installing Graphviz (optional):**\n",
    "* Mac:\n",
    "    * [Download and install PKG file](http://www.graphviz.org/Download_macos.php)\n",
    "* Windows:\n",
    "    * [Download and install MSI file](http://www.graphviz.org/Download_windows.php)\n",
    "    * Add it to your Path: Go to Control Panel, System, Advanced System Settings, Environment Variables. Under system variables, edit \"Path\" to include the path to the \"bin\" folder, such as: `C:\\Program Files (x86)\\Graphviz2.38\\bin`\n",
    "__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Adapted from Chapter 8 of [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)*\n",
    "\n",
    "||continuous|categorical|\n",
    "|---|---|---|\n",
    "|**supervised**|**regression**|**classification**|\n",
    "|**unsupervised**|dimension reduction|clustering|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Identify various methods of measuring purity\n",
    "* Learn how trees are build for both regressions and classification\n",
    "* Use the ROC curve to understand the depth of classification error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Building a regression tree by hand\n",
    "\n",
    "How do you build a decision tree? You're going to find out by building one in pairs!\n",
    "\n",
    "Your training data is a tiny dataset of [used vehicle sale prices](https://raw.githubusercontent.com/justmarkham/DAT4/master/data/used_vehicles.csv). Your goal is to predict Price for out-of-sample data. Here are your instructions:\n",
    "\n",
    "- Read the data into Pandas.\n",
    "- Explore the data by sorting, plotting, or split-apply-combine (aka `group_by`).\n",
    "- Decide which feature is the most important predictor, and use that to make your first split. (Only binary splits are allowed!)\n",
    "- After making your first split, you should actually split your data in Pandas into two parts, and then explore each part to figure out what other splits to make.\n",
    "- Stop making splits once you are convinced that it strikes a good balance between underfitting and overfitting. (As always, your goal is to build a model that generalizes well!)\n",
    "- You are allowed to split on the same variable multiple times!\n",
    "- Draw your tree, making sure to label your leaves with the mean Price for the observations in that \"bucket\".\n",
    "- When you're finished, review your tree to make sure nothing is backwards. (Remember: follow the left branch if the rule is true, and follow the right branch if the rule is false.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# read in vehicle data\n",
    "vehicles = pd.read_csv('../data/used_vehicles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Your code here:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does a computer build a regression tree?\n",
    "\n",
    "The ideal approach would be for the computer to consider every possible partition of the feature space. However, this is computationally infeasible, so instead an approach is used called **recursive binary splitting:**\n",
    "\n",
    "- Begin at the top of the tree.\n",
    "- For every single predictor, examine every possible cutpoint, and choose the predictor and cutpoint such that the resulting tree has the **lowest possible mean squared error (MSE)**. Make that split.\n",
    "- Repeat the examination for the two resulting regions, and again make a single split (in one of the regions) to minimize the MSE.\n",
    "- Keep repeating this process until a stopping criteria is met.\n",
    "\n",
    "**How does it know when to stop?**\n",
    "\n",
    "1. We could define a stopping criterion, such as a **maximum depth** of the tree or the **minimum number of samples in the leaf**.\n",
    "2. We could grow the tree deep, and then \"prune\" it back using a method such as \"cost complexity pruning\" (aka \"weakest link pruning\").\n",
    "\n",
    "Method 2 involves setting a tuning parameter that penalizes the tree for having too many leaves. As the parameter is increased, branches automatically get pruned from the tree, resulting in smaller and smaller trees. The tuning parameter can be selected through cross-validation.\n",
    "\n",
    "Note: **Method 2 is not currently supported by scikit-learn**, and so we will use Method 1 instead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a regression tree in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>miles</th>\n",
       "      <th>doors</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22000</td>\n",
       "      <td>2012</td>\n",
       "      <td>13000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14000</td>\n",
       "      <td>2010</td>\n",
       "      <td>30000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13000</td>\n",
       "      <td>2010</td>\n",
       "      <td>73500</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9500</td>\n",
       "      <td>2009</td>\n",
       "      <td>78000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9000</td>\n",
       "      <td>2007</td>\n",
       "      <td>47000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4000</td>\n",
       "      <td>2006</td>\n",
       "      <td>124000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3000</td>\n",
       "      <td>2004</td>\n",
       "      <td>177000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2000</td>\n",
       "      <td>2004</td>\n",
       "      <td>209000</td>\n",
       "      <td>4</td>\n",
       "      <td>truck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3000</td>\n",
       "      <td>2003</td>\n",
       "      <td>138000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1900</td>\n",
       "      <td>2003</td>\n",
       "      <td>160000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2500</td>\n",
       "      <td>2003</td>\n",
       "      <td>190000</td>\n",
       "      <td>2</td>\n",
       "      <td>truck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5000</td>\n",
       "      <td>2001</td>\n",
       "      <td>62000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1800</td>\n",
       "      <td>1999</td>\n",
       "      <td>163000</td>\n",
       "      <td>2</td>\n",
       "      <td>truck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1300</td>\n",
       "      <td>1997</td>\n",
       "      <td>138000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    price  year   miles  doors   type\n",
       "0   22000  2012   13000      2    car\n",
       "1   14000  2010   30000      2    car\n",
       "2   13000  2010   73500      4    car\n",
       "3    9500  2009   78000      4    car\n",
       "4    9000  2007   47000      4    car\n",
       "5    4000  2006  124000      2    car\n",
       "6    3000  2004  177000      4    car\n",
       "7    2000  2004  209000      4  truck\n",
       "8    3000  2003  138000      2    car\n",
       "9    1900  2003  160000      4    car\n",
       "10   2500  2003  190000      2  truck\n",
       "11   5000  2001   62000      4    car\n",
       "12   1800  1999  163000      2  truck\n",
       "13   1300  1997  138000      4    car"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# read in vehicle data\n",
    "vehicles = pd.read_csv('../data/used_vehicles.csv')\n",
    "\n",
    "# print out data\n",
    "vehicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valeriav/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "/Users/valeriav/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# convert car to 0 and truck to 1\n",
    "#vehicles['type'] = vehicles.type.map({'car':0, 'truck':1})\n",
    "vehicles['type'][vehicles['type']=='car']=0\n",
    "vehicles['type'][vehicles['type']=='truck']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# select feature columns (every column except for the 0th column)\n",
    "feature_cols = vehicles.columns[1:]\n",
    "\n",
    "# define X (features) and y (response)\n",
    "X = vehicles[feature_cols]\n",
    "y = vehicles.price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split into train/test\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    year   miles  doors type\n",
      "10  2003  190000      2    1\n",
      "4   2007   47000      4    0\n",
      "1   2010   30000      2    0\n",
      "12  1999  163000      2    1\n",
      "0   2012   13000      2    0\n",
      "13  1997  138000      4    0\n",
      "9   2003  160000      4    0\n",
      "8   2003  138000      2    0\n",
      "11  2001   62000      4    0\n",
      "5   2006  124000      2    0\n"
     ]
    }
   ],
   "source": [
    "# print out each of the arrays\n",
    "print X_train\n",
    "# print y_train\n",
    "# print X_test\n",
    "# print y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, random_state=1, splitter='best')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import class, instantiate estimator, fit with training set\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "treereg = DecisionTreeRegressor(random_state=1)\n",
    "treereg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5000.  1900.  1900.  5000.]\n",
      "3     9500\n",
      "7     2000\n",
      "6     3000\n",
      "2    13000\n",
      "Name: price, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "preds = treereg.predict(X_test)\n",
    "\n",
    "# print predictions and actual values\n",
    "print preds\n",
    "print y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4622.4993239588475"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print RMSE\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "np.sqrt(metrics.mean_squared_error(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `cross_val_score`\n",
    "\n",
    "`cross_val_score` is a function in sklearn that allows us to generate cross validation scores more programatically.\n",
    "\n",
    "\n",
    "With `cross_val_score`, we can assume a k fold given X and y, and pass in a string for scoring.  \n",
    "This should completely simplify our process:\n",
    "\n",
    "```python\n",
    "scores = cross_val_score(treereg, X, y, cv=3, scoring='accuracy_score')\n",
    "```\n",
    "\n",
    "the scoring string should match any function from the [metrics](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) module:\n",
    "\n",
    "```python\n",
    "['accuracy', 'adjusted_rand_score', 'average_precision', 'f1', 'log_loss', 'mean_absolute_error', 'mean_squared_error', 'precision', 'r2', 'recall', 'roc_auc']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use cross-validation to find best max_depth\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4804.3767888427128"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try max_depth=2\n",
    "treereg = DecisionTreeRegressor(max_depth=2, random_state=1)\n",
    "scores = cross_val_score(treereg, X, y, cv=3, scoring='mean_squared_error')\n",
    "np.mean(np.sqrt(-scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4592.1554255755254"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try max_depth=3\n",
    "treereg = DecisionTreeRegressor(max_depth=3, random_state=1)\n",
    "scores = cross_val_score(treereg, X, y, cv=3, scoring='mean_squared_error')\n",
    "np.mean(np.sqrt(-scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4704.0052694797387"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try max_depth=4\n",
    "treereg = DecisionTreeRegressor(max_depth=4, random_state=1)\n",
    "scores = cross_val_score(treereg, X, y, cv=3, scoring='mean_squared_error')\n",
    "np.mean(np.sqrt(-scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=3, max_features=None,\n",
       "           max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, random_state=1, splitter='best')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_depth=3 was best, so fit a tree using that parameter with ALL DATA\n",
    "treereg = DecisionTreeRegressor(max_depth=3, random_state=1)\n",
    "treereg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, we can solve (and plot) for this more programattically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named seaborn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-ababbb057e10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_style\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'whitegrid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named seaborn"
     ]
    }
   ],
   "source": [
    "% matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "all_scores = []\n",
    "best_score = -1\n",
    "best_depth = 0\n",
    "for i in range(1, 6):\n",
    "    treereg = DecisionTreeRegressor(max_depth=i, random_state=1)\n",
    "    scores = cross_val_score(treereg, X, y, cv=3, scoring='mean_squared_error')\n",
    "    current_score = np.mean(np.sqrt(-scores))\n",
    "    # If the score mean is better than the current best, or best is the default (-1), then update!\n",
    "    if current_score < best_score or best_score == -1:\n",
    "        best_score = current_score\n",
    "        best_depth = i\n",
    "    # store to plot anyway!\n",
    "    all_scores.append(current_score)\n",
    "    \n",
    "print \"Best score: %s\" % best_score\n",
    "print \"Best depth: %s\" % best_depth\n",
    "\n",
    "# now actually fit the model\n",
    "treereg = DecisionTreeRegressor(max_depth=best_depth, random_state=1)\n",
    "treereg.fit(X, y)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(1, 6), all_scores)\n",
    "plt.xlabel('x=max tree depth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>year</td>\n",
       "      <td>0.798744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>miles</td>\n",
       "      <td>0.201256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doors</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>type</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  feature  importance\n",
       "0    year    0.798744\n",
       "1   miles    0.201256\n",
       "2   doors    0.000000\n",
       "3    type    0.000000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the \"Gini importance\" of each feature: the (normalized) total reduction of MSE brought by that feature\n",
    "pd.DataFrame({'feature':feature_cols, 'importance':treereg.feature_importances_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a Graphviz file\n",
    "from sklearn.tree import export_graphviz\n",
    "with open(\"15_vehicles.dot\", 'wb') as f:\n",
    "    f = export_graphviz(treereg, out_file=f, feature_names=feature_cols)\n",
    "\n",
    "# at the command line, run this to convert to PNG:\n",
    "#dot -Tpng 15_vehicles.dot -o 15_vehicles.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/15_vehicles_instructor.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting a tree diagram\n",
    "\n",
    "How do we read this decision tree?\n",
    "\n",
    "**Internal nodes:**\n",
    "\n",
    "- \"samples\" is the number of observations in that node before splitting\n",
    "- \"mse\" is the mean squared error calculated by comparing the actual response values in that node against the mean response value in that node\n",
    "- first line is the condition used to split that node (go left if true, go right if false)\n",
    "\n",
    "**Leaves:**\n",
    "\n",
    "- \"samples\" is the number of observations in that node\n",
    "- \"value\" is the mean response value in that node\n",
    "- \"mse\" is the mean squared error calculated by comparing the actual response values in that node against \"value\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises: \n",
    "\n",
    "\n",
    "### Exercise 2: Use GridSearchCV to find te best Regression Tree\n",
    "\n",
    "How do we know by pruning with max depth is the best model for us? Trees offer a variety of ways to pre-prune (that is, we tell a computer how to design the resulting tree with certain \"gotchas\").\n",
    "\n",
    "Measure           | What it does\n",
    "------------------|-------------\n",
    "max_depth         | How many nodes deep can the decision tree go?\n",
    "max_features      | Is there a cut off to the number of features to use?\n",
    "max_leaf_nodes    | How many leaves can be generated per node?\n",
    "min_samples_leaf  | How many samples need to be included at a leaf, at a minimum?  \n",
    "min_samples_split | How many samples need to be included at a node, at a minimum?\n",
    "\n",
    "While the data set is small, experiment with each in a loop and explain what is occuring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Your code here:\n",
    "\n",
    "PARAMETERS = {}\n",
    "SCORING = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PARAMETERS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-a2b4933d21ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#Grid Search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTreeRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPARAMETERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSCORING\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PARAMETERS' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import grid_search\n",
    "\n",
    "#Grid Search\n",
    "model = DecisionTreeRegressor()\n",
    "clf = grid_search.GridSearchCV(model, parameters=PARAMETERS, scoring=SCORING, verbose=True, n_jobs=1)\n",
    "clf.fit(X, y)\n",
    "\n",
    "#After completion, show the final best results and scores\n",
    "print clf.best_estimator_\n",
    "print clf.best_score_\n",
    "print np.sqrt(-clf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execrise 3: Predicting for out-of-sample data\n",
    "\n",
    "How accurate is scikit-learn's regression tree at predicting the out-of-sample data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in out-of-sample data\n",
    "oos = pd.read_csv('../data/used_vehicles_oos.csv')\n",
    "\n",
    "# convert car to 0 and truck to 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define X and y\n",
    "X_oos = \n",
    "y_oos = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make predictions on out-of-sample data\n",
    "# preds = \n",
    "\n",
    "# print predictions and actual values\n",
    "print \"Predicted Values: {}\".format(preds)\n",
    "print \"Actual Values:    {}\".format(y_oos.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print RMSE\n",
    "np.sqrt(metrics.mean_squared_error(y_oos, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________\n",
    "### Classification trees\n",
    "\n",
    "Classification trees are very similar to regression trees. Here is a quick comparison:\n",
    "\n",
    "|regression trees|classification trees|\n",
    "|---|---|\n",
    "|predict a continuous response|predict a categorical response|\n",
    "|predict using mean response of each leaf|predict using most commonly occuring class of each leaf|\n",
    "|splits are chosen to minimize MSE|splits are chosen to minimize a different criterion (discussed below)|\n",
    "\n",
    "Note that classification trees easily handle **more than two response classes**! (How have other classification models we've seen handled this scenario?)\n",
    "\n",
    "Here's an **example of a classification tree**, which predicts whether or not a patient who presented with chest pain has heart disease:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a classification tree in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll build a classification tree using the [Titanic data](https://www.kaggle.com/c/titanic-gettingStarted/data) provided by Kaggle.\n",
    "\n",
    "Note: this is the same data we used for the Midterm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in the data\n",
    "titanic = pd.read_csv('../data/titanic.csv')\n",
    "titanic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# look for missing values\n",
    "titanic.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's choose our response and a few features, and decide whether we need to adjust them:\n",
    "\n",
    "- **survived:** This is our response, and is already encoded as 0=died and 1=survived.\n",
    "- **pclass:** These are the passenger class categories (1=first class, 2=second class, 3=third class). Should we consider these ordered or nonordered?\n",
    "- **sex:** This is a binary category, so we should encode as 0=female and 1=male.\n",
    "- **age:** We need to fill in the missing values.\n",
    "- **embarked:** This is the port they emarked from. There are three unordered categories, so we'll create dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# copy\n",
    "titanic_c = pd.DataFrame(titanic)\n",
    "\n",
    "# encode sex feature\n",
    "titanic_c['sex'] = titanic.sex.map({'female':0, 'male':1})\n",
    "\n",
    "# fill in missing values for age - there are much more intelligent ways to handle this... but for today:\n",
    "titanic_c.age.fillna(titanic.age.mean(), inplace=True)\n",
    "\n",
    "# print the updated DataFrame\n",
    "titanic_c.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create three dummy variables using get_dummies\n",
    "pd.get_dummies(titanic_c.embarked, prefix='embarked').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # create three dummy variables, drop the first dummy variable, and store this as a DataFrame\n",
    "embarked_dummies = pd.get_dummies(titanic_c.embarked, prefix='embarked').iloc[:, 1:]\n",
    "\n",
    "# # join the two dummy variable columns onto the original DataFrame\n",
    "titanic_c = titanic_c.join(embarked_dummies)\n",
    "\n",
    "# # print the updated DataFrame\n",
    "titanic_c.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a list of feature columns\n",
    "feature_cols = ['pclass', 'sex', 'age', 'embarked_Q', 'embarked_S']\n",
    "\n",
    "# define X and y\n",
    "X = titanic_c[feature_cols]\n",
    "y = titanic_c.survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fit a classification tree with max_depth=3 on all data\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "treeclf = DecisionTreeClassifier(max_depth=3, random_state=1)\n",
    "treeclf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a Graphviz file\n",
    "with open(\"15_titanic.dot\", 'wb') as f:\n",
    "    f = export_graphviz(treeclf, out_file=f, feature_names=feature_cols)\n",
    "    \n",
    "#from the command line run: \n",
    "#dot -Tpng 15_titanic.dot -o 15_titanic.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/15_titanic_instructor.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the split in the bottom right, which was made only to increase node purity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute the feature importances\n",
    "pd.DataFrame({'feature':feature_cols, 'importance':treeclf.feature_importances_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[524  25]\n",
      " [133 209]]\n"
     ]
    }
   ],
   "source": [
    "# Compute the confusion matrix\n",
    "conf = metrics.confusion_matrix(y, treeclf.predict(X))\n",
    "print conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on Confusion Matrices (one last metric for classification)\n",
    "\n",
    "Arguably the most useful metric we can use in binary classification problems is the Recieving Opererating Characteristic (ROC) curve, or in particular, the Area Under said Curve (AUC). We can calculate the AUC from the results of a confusion matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/confusion_matrix_metrics.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[209  25]\n",
      " [133 524]]\n",
      "fpr 0.0455373406193\n",
      "tpr 0.611111111111\n",
      "precision 0.893162393162\n",
      "accuracy 0.822671156004\n"
     ]
    }
   ],
   "source": [
    "## THE CONFUSION MATRIX FROM SKLEARN IS INVERTED. (rows = true, cols = predicted)\n",
    "from __future__ import division\n",
    "def true_confusion_matrix(mtrx):\n",
    "    return np.array([[mtrx[1, 1], mtrx[0, 1]], [mtrx[1, 0], mtrx[0, 0]]])\n",
    "\n",
    "true_conf = true_confusion_matrix(conf)\n",
    "\n",
    "print true_conf\n",
    "# false positive rate (fpr) = false positives / (false positives + true negatives)\n",
    "# \"What percent of the negatives were predicted as positive?\"\n",
    "# Goal: keep this low\n",
    "def fpr(confusion):\n",
    "    return confusion[0, 1] / (confusion[0, 1] + confusion[1, 1])\n",
    "\n",
    "print 'fpr', fpr(true_conf)\n",
    "\n",
    "# true positive rate/recall (tpr) = true positives / (true positives + false negatives)\n",
    "# \"What percent of the positives were accurately measured as positives?\"\n",
    "# Goal: keep this high\n",
    "def tpr(confusion):\n",
    "    return confusion[0, 0] / (confusion[0, 0] + confusion[1, 0])\n",
    "\n",
    "print 'tpr', tpr(true_conf)\n",
    "\n",
    "\n",
    "# precision = true positives / (true positives + false positives)\n",
    "# \"What percent of predicted positives were truly positive?\n",
    "# Goal: keep this high\n",
    "def precision(confusion):\n",
    "    return confusion[0, 0] / (confusion[0, 0] + confusion[0, 1])\n",
    "\n",
    "print 'precision', precision(true_conf)\n",
    "    \n",
    "# accuracy = (true positives + true negatives) / total observations\n",
    "# \"What percent of values were accurately predicted?\"\n",
    "# Goal: Keep this high\n",
    "def acc(confusion):\n",
    "    return (confusion[0, 0] + confusion[1, 1]) / (confusion[0, 0] + confusion[1, 1] + confusion[0, 1] + confusion[1, 0])\n",
    "\n",
    "print 'accuracy', acc(true_conf)\n",
    "    \n",
    "# auc = calculate area under x=fpr, y=tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fpr 0.0455373406193\n",
      "tpr 0.611111111111\n",
      "precision 0.893162393162\n",
      "accuracy 0.822671156004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x10a294c50>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfYAAAFkCAYAAADSRRn0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIhHy/wMauUCRLDTPZgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a29c990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sklearn has all of this built in:\n",
    "\n",
    "print 'fpr', metrics.roc_curve(y, treeclf.predict(X))[0][1] #fpr\n",
    "print 'tpr', metrics.roc_curve(y, treeclf.predict(X))[1][1] #tpr\n",
    "print 'precision', metrics.precision_score(y, treeclf.predict(X))\n",
    "print 'accuracy', metrics.accuracy_score(y, treeclf.predict(X))\n",
    "\n",
    "roc = metrics.roc_curve(y, treeclf.predict(X))\n",
    "plt.figure()\n",
    "# dummy, what's the worst random job we can do?\n",
    "plt.plot([0, 0.5, 1], [0, 0.5, 1])\n",
    "plt.plot(roc[0], roc[1])\n",
    "plt.xlabel('false positive rate')\n",
    "plt.ylabel('true positive rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independent / On your Own\n",
    "\n",
    "Like we had done early optimizing the model, here are a few things we should try to make sure we've fit our best model:\n",
    "\n",
    "1. Play with the stopping criteria in a loop and determine which produces the best cross validated model.\n",
    "2. Compare your best tree model to another classifier (naive bayes, logistic regression). Which seems to perform the best, and why do you think so?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping up decision trees\n",
    "\n",
    "Here are some advantages and disadvantages of decision trees that we haven't yet talked about:\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Can be specified as a series of rules, and are thought to more closely approximate human decision-making than other models\n",
    "- Non-parametric (will do better than linear regression if relationship between predictors and response is highly non-linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disadvantages:**\n",
    "\n",
    "- Small variations in the data can result in a completely different tree\n",
    "- Recursive binary splitting makes \"locally optimal\" decisions that may not result in a globally optimal tree\n",
    "- Can create biased trees if the classes are highly imbalanced\n",
    "\n",
    "Note that there is not just one decision tree algorithm; instead, there are many variations. A few common decision tree algorithms that are often referred to by name are C4.5, C5.0, and CART. (More details are available in the [scikit-learn documentation](http://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart).) scikit-learn uses an \"optimized version\" of CART."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "**Decision Trees**\n",
    "\n",
    "- scikit-learn documentation: [Decision Trees](http://scikit-learn.org/stable/modules/tree.html)\n",
    "- Additional thoughts on [Gini Vs Entropy](http://www.garysieling.com/blog/sklearn-gini-vs-entropy-criteria)\n",
    "\n",
    "** ROC **\n",
    "- [Really Awesome Paper!](https://ccrma.stanford.edu/workshops/mir2009/references/ROCintro.pdf)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
